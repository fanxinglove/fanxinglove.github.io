{"title":"scrapy学习笔记","slug":"2019-02-22-scrapy学习笔记","date":"2019-02-21T16:00:00.000Z","updated":"2019-03-05T11:40:41.241Z","comments":true,"excerpt":"","content":"<blockquote>\n<p>学习scrapy框架的一些个人笔记</p>\n</blockquote>\n<h1 id=\"scrapy学习笔记-一\"><a href=\"#scrapy学习笔记-一\" class=\"headerlink\" title=\"scrapy学习笔记(一)\"></a>scrapy学习笔记(一)</h1><h4 id=\"安装scrapy-windows\"><a href=\"#安装scrapy-windows\" class=\"headerlink\" title=\"安装scrapy(windows)\"></a>安装scrapy(windows)</h4><pre><code>1.下载Twisted:https://www.lfd.uci.edu/~gohlke/pythonlibs/\n2.pip install (Twisted下载的版本)\n3.pip install scrapy\n</code></pre><h4 id=\"scrapy创建项目\"><a href=\"#scrapy创建项目\" class=\"headerlink\" title=\"scrapy创建项目\"></a>scrapy创建项目</h4><pre><code>scrapy startproject [项目名字](fanxing)\n\nscrapy.cfg:项目的配置文件\ntutorial/: 该项目的python模块。之后您将在此加入代码。\ntutorial/items.py:项目中的item文件\ntutorial/pipelines.py:项目中的pipelines文件\ntutorial/settings.py:项目的设置文件\nspiders:放置spider代码的目录\n</code></pre><p>scrapy目录结构:<br><img src=\"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1550865769357&amp;di=1202b05b75058fa2fa3b385e4fd32ada&amp;imgtype=0&amp;src=http%3A%2F%2Fimg2018.cnblogs.com%2Fblog%2F1059579%2F201809%2F1059579-20180917171719566-1601375189.png\" alt></p>\n<h4 id=\"编写爬虫\"><a href=\"#编写爬虫\" class=\"headerlink\" title=\"编写爬虫\"></a>编写爬虫</h4><pre><code>cd fanxing  进入项目名字\nscrapy genspider fx_spider(爬虫名字) www.ke.qq.com(域名)  创建爬虫\nscrapy crawl fx_spider  运行爬虫\n</code></pre><h4 id=\"fx-spider编写获取的数据-利用xpath或者css\"><a href=\"#fx-spider编写获取的数据-利用xpath或者css\" class=\"headerlink\" title=\"fx_spider编写获取的数据(利用xpath或者css)\"></a>fx_spider编写获取的数据(利用xpath或者css)</h4><pre><code># -*- coding: utf-8 -*-\nimport scrapy\n\nclass FxSpiderSpider(scrapy.Spider):\n    name = &apos;tz_spider&apos;\n    # allowed_domains = [&apos;ke.qq.com&apos;]\n    start_urls = [&apos;http://ke.qq.com/course/list?mt=100&apos;]\n\n    def parse(self, response):\n        pass\n</code></pre><h4 id=\"Item类，明确爬取数据\"><a href=\"#Item类，明确爬取数据\" class=\"headerlink\" title=\"Item类，明确爬取数据\"></a>Item类，明确爬取数据</h4><pre><code>import scrapy\n\nclass FanxingspiderItem(scrapy.Item):\n    title = scrapy.Field()\n    school_name = scrapy.Field()\n# 定义好Itme后，需要在fx_spider导入包：from ..items import FanxingspiderItem\n</code></pre><h4 id=\"Pipeline对item声明的数据进行处理\"><a href=\"#Pipeline对item声明的数据进行处理\" class=\"headerlink\" title=\"Pipeline对item声明的数据进行处理\"></a>Pipeline对item声明的数据进行处理</h4><pre><code>import json\n\nclass FxSpiderPipeline(object):\n    def open_spider(self,spider):\n        # 爬虫启动时打开文件\n        self.f = open(&apos;fx.json&apos;,&apos;a+&apos;,encoding=&apos;utf-8&apos;)\n\n    def process_item(self,item,spider):\n        # 将数据写入传递过来的item\n        self.write(json.dumps(dict(item),ensure_ascii=False) + &apos;\\n&apos;)\n\n    def close_spider(self,spider):\n        # 爬虫关闭时，关闭文件\n        self.f.close()\n</code></pre><p>要激活管道组件，必须添加到ITEM_PIPELINES中，settings.py中设置</p>\n<pre><code>ITEM_PIPELINES = {\n   &apos;TanzhouSpider.pipelines.TanzhouspiderPipeline&apos;: 300,\n}\n</code></pre><h1 id=\"scrapy学习笔记-二\"><a href=\"#scrapy学习笔记-二\" class=\"headerlink\" title=\"scrapy学习笔记(二)\"></a>scrapy学习笔记(二)</h1><h4 id=\"进入scrapy-shell\"><a href=\"#进入scrapy-shell\" class=\"headerlink\" title=\"进入scrapy shell\"></a>进入scrapy shell</h4><pre><code>scrapy shell [url]\n</code></pre><h4 id=\"CrawlSpider\"><a href=\"#CrawlSpider\" class=\"headerlink\" title=\"CrawlSpider\"></a>CrawlSpider</h4><pre><code>创建CrawlSpider：scrapy genspider -t crawl [url](要爬取的链接)\n</code></pre><h5 id=\"Rule规则\"><a href=\"#Rule规则\" class=\"headerlink\" title=\"Rule规则\"></a>Rule规则</h5><p>CrawlSpider爬虫比spider多了rules，多了一些规则，如下：</p>\n<pre><code># -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\n\nclass NewSpiderSpider(CrawlSpider):\n    name = &apos;new_spider&apos;\n    start_urls = [&apos;https://www.163.com/&apos;]\n\n    rules = (\n        Rule(LinkExtractor(allow=r&apos;Items/&apos;), callback=&apos;parse_item&apos;, follow=True),\n    )\n\n    def parse_item(self, response):\n        i = {}\n        return i\n</code></pre><p>Rule用来定义CrawlSpider爬取规则，参数有：</p>\n<pre><code>link_extractor  Link Extractor对象，它定义如何从每个已爬网页面中提取链接。\ncallback        回调函数\ncb_kwargs       是一个包含要传递给回调函数的关键字参数的dict\nfollow          它指定是否应该从使用此规则提取的每个响应中跟踪链接。(True为进一步提取)\nprocess_links   用于过滤连接的回调函数\nprocess_request 用于过滤请求的额回调函数\n</code></pre><h4 id=\"LinkExtractor链接提取器\"><a href=\"#LinkExtractor链接提取器\" class=\"headerlink\" title=\"LinkExtractor链接提取器\"></a>LinkExtractor链接提取器</h4><pre><code>allow：提取满足正则表达式的链接\ndeny：排除正则表达式匹配的链接（优先级高于allow）\nallow_domains：允许的域名（可以是str或list）\ndeny_domains：排除的域名（可以是str或list）\nrestrict_xpaths：提取满足XPath选择条件的链接（可以是str或list）\nrestrict_css：提取满足css选择条件的链接（可以是str或list）\ntags：提取指定标签下的链接，默认从a和area中提取（可以是str或list）\nattrs：提取满足拥有属性的链接，默认为href（类型为list）\nunique：链接是否去重（类型为boolean）\nprocess_value：值处理函数（优先级大于allow\n</code></pre><h1 id=\"scrapy学习笔记-三\"><a href=\"#scrapy学习笔记-三\" class=\"headerlink\" title=\"scrapy学习笔记(三)\"></a>scrapy学习笔记(三)</h1><h4 id=\"Request对象\"><a href=\"#Request对象\" class=\"headerlink\" title=\"Request对象\"></a>Request对象</h4><p>Scrapy.http.Request类是scrapy框架中request的基类，参数有:</p>\n<pre><code>url（字符串） - 此请求的URL\ncallback（callable）- 回调函数\nmethod（string） - 此请求的HTTP方法。默认为&apos;GET&apos;。\nmeta（dict） - Request.meta属性的初始值。\nbody（str 或unicode） - 请求体。如果没有传参，默认为空字符串。\nheaders（dict） - 此请求的请求头。\ncookies - 请求cookie。\nencoding（字符串） - 此请求的编码（默认为&apos;utf-8&apos;）此编码将用于对URL进行百分比编码并将body转换为str（如果给定unicode）。\npriority（int） - 此请求的优先级（默认为0）。\ndont_filter（boolean） - 表示调度程序不应过滤此请求。\nerrback（callable） - 在处理请求时引发任何异常时将调用的函数。\nflags（list） - 发送给请求的标志，可用于日志记录或类似目的。\n</code></pre><p>Request对象属性和方法</p>\n<pre><code>url 包含此请求的URL的字符串。该属性是只读的。更改请求使用的URL replace()。\nmethod  表示请求中的HTTP方法的字符串。\nheaders 类似字典的对象，包含请求头。\nbody 包含请求正文的str。该属性是只读的。更改请求使用的URL replace()。\nmeta 包含此请求的任意元数据的字典。\ncopy() 返回一个新的请求，改请求是此请求的副本。\nreplace（[ URL，method，headers，body，cookies，meta，encoding，dont_filter，callback，errback] ） 返回一个更新对的request\n</code></pre><h4 id=\"FromRequest\"><a href=\"#FromRequest\" class=\"headerlink\" title=\"FromRequest\"></a>FromRequest</h4><p>FromRequest具有处理HTML表单的功能，是Request的基类，通过FromRequest发送post数据：</p>\n<pre><code># -*- coding: utf-8 -*-\nimport scrapy\n\nclass HbSpiderSpider(scrapy.Spider):\n    name = &apos;hb_spider&apos;\n    start_urls = []\n\n    def start_requests(self):\n        url = &apos;http://httpbin.org/post&apos;\n        data = {\n            &apos;username&apos;: &apos;root&apos;,\n            &apos;password&apos;: &apos;123456&apos;\n        }\n        yield scrapy.FormRequest(url=url,formdata=data,callback=self.parse)\n\n    def parse(self, response):\n        print(response.text)\n</code></pre><h4 id=\"Response对象\"><a href=\"#Response对象\" class=\"headerlink\" title=\"Response对象\"></a>Response对象</h4><pre><code>url:HTTP 响应的url地址，str 类型。\nstatus:表示响应的HTTP状态的整数\nheader:HTTP 响应的头部，dict 类型。可以调用get或getlist方法对其进行访问。\nmeta:从其他请求传过来的meta属性，可以用来保持多个请求之间的数据链接。\nencoding:返回当前字符串编码和解码的格式。\ntext:将返回来的数据作为unicode字符串返回。\nbody:将返回来的数据作为bytes字符串返回。\nxpath:xpath选择器。\ncss:css选择器。\n</code></pre><h4 id=\"scrapy日志使用\"><a href=\"#scrapy日志使用\" class=\"headerlink\" title=\"scrapy日志使用\"></a>scrapy日志使用</h4><p>这些设置可以用于日志记录:</p>\n<pre><code>LOG_FILE        日志输出文件，如果为None，就打印在控制台\nLOG_ENABLED    是否启用日志，默认True\nLOG_ENCODING       日期编码，默认utf-8\nLOG_LEVEL    日志等级，默认debug\nLOG_FORMAT    日志格式\nLOG_DATEFORMAT    日志日期格式\nLOG_STDOUT    日志标准输出，默认False，如果True所有标准输出都将写入日志中\nLOG_SHORT_NAMES   短日志名，默认为False，如果True将不输出组件名\n</code></pre><p>一般都只设置这些(在settings中设置):</p>\n<pre><code>LOG_FILE = &apos;文件名字&apos;\nLOG_LEVEL = &apos;日志等级&apos;\n</code></pre><h1 id=\"scrapy学习笔记-四\"><a href=\"#scrapy学习笔记-四\" class=\"headerlink\" title=\"scrapy学习笔记(四)\"></a>scrapy学习笔记(四)</h1><h4 id=\"下载中间件API-Downloader-Middlewares\"><a href=\"#下载中间件API-Downloader-Middlewares\" class=\"headerlink\" title=\"下载中间件API(Downloader Middlewares)\"></a>下载中间件API(Downloader Middlewares)</h4><pre><code>process_request(request,spider)：处理请求，对于通过中间件的每个请求调用此方法\nprocess_response(request,response,spider)：处理响应，对于通过中间件的每个响应，调用此方法\nprocess_exception(request,exception,spider)：处理请求时发生了异常调用\nfrom_crawler(cls,crawler)：从crawler调用\n</code></pre><h4 id=\"process-request-self-request-spider\"><a href=\"#process-request-self-request-spider\" class=\"headerlink\" title=\"process_request(self,request,spider)\"></a>process_request(self,request,spider)</h4><pre><code>1.参数\n~  request : 发送请求的request对象\n~  spider : 发送请求的spider对象\n2.返回值\n~ 返回None : 如果返回None，Scrapy将继续处理该request，执行其他中间件中的相应方法，直到合适的下载器处理函数被调用。\n~ 返回Response对象 : Scrapy将不会调用任何其他的process_request方法，将直接返回这个response。已经激活的中间件的process_response()方法则会在每个response返回时被调用。\n~ 返回Request对象 : 不再使用之前的request对象去下载数据，而是根据现在返回的request对象返回数据。\n~ 如果这个方法中抛出了异常，则会调用process_exception方法。\n</code></pre><h4 id=\"process-response-self-response-spider\"><a href=\"#process-response-self-response-spider\" class=\"headerlink\" title=\"process_response(self,response,spider)\"></a>process_response(self,response,spider)</h4><pre><code>1.参数\n~  request : request对象。\n~  response : 被处理的response对象。\n~  spider : spider对象。\n2.返回值\n~ 返回Response对象 : 会将这个新的response对象传给其他中间件，最终传给爬虫。\n~ 返回Request对象 : 下载器链接切断，返回的request会重新被下载器调度下载。\n~ 如果抛出一个异常，那么调用request的errback方法，如果没有指定这个方法，那么会抛出一个异常。\n</code></pre><h4 id=\"设置随机请求头\"><a href=\"#设置随机请求头\" class=\"headerlink\" title=\"设置随机请求头\"></a>设置随机请求头</h4><pre><code>import random\n\nUSER_AGENT = [\n    &apos;User-Agent:Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&apos;,\n    &apos;User-Agent:Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&apos;,\n    &apos;User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)&apos;\n]\n\nclass UserAgentMiddleware(object):\n    def process_request(self,request,spider):\n        request.headers[&apos;USER-AGENT&apos;] = random.choice(USER_AGENT)\n\n    def process_response(self,request,response,spider):\n        if response.status == 403:\n            print(&apos;请求失败...&apos;)\n            return request\n        else:\n            print(&apos;请求成功&apos;)\n            return response\n</code></pre><h1 id=\"scrapy学习笔记-五\"><a href=\"#scrapy学习笔记-五\" class=\"headerlink\" title=\"scrapy学习笔记(五)\"></a>scrapy学习笔记(五)</h1><h4 id=\"scrapy下载图片\"><a href=\"#scrapy下载图片\" class=\"headerlink\" title=\"scrapy下载图片\"></a>scrapy下载图片</h4><p>spider编写：</p>\n<pre><code># -*- coding: utf-8 -*-\nimport scrapy\nfrom ..items import DoutuItem\n\nclass DtSpiderSpider(scrapy.Spider):\n    name = &apos;dt_spider&apos;\n    # allowed_domains = [&apos;dddd&apos;]\n    start_urls = [&apos;http://www.doutula.com&apos;]\n\n    def parse(self, response):\n        # 获取图片url\n        image_urls = response.xpath(&apos;//a[@class=&quot;col-xs-6 col-sm-3&quot;]/img/@data-original&apos;).extract()\n        for image_url in image_urls:\n            items = DoutuItem()\n            items[&apos;image_url&apos;] = image_url\n            yield items\n</code></pre><p>item编写:</p>\n<pre><code>import scrapy\n\nclass DoutuItem(scrapy.Item):\n    image_url = scrapy.Field()\n</code></pre><p>pipelines编写:</p>\n<pre><code># -*- coding: utf-8 -*-\n\nimport scrapy\nfrom scrapy.pipelines.images import ImagesPipeline\n\n# class DoutuPipeline(object):\n#     def process_item(self, item, spider):\n#         return item\n\n# 下载图片，需要重写ImagePipelines类\nclass DouTuImagePipelines(ImagesPipeline):\n    # 对item里面的url生成Request请求\n    def get_media_requests(self,item,info):\n        url = item[&apos;image_url&apos;]\n        return scrapy.Request(url=url)\n</code></pre><p>settings编写:</p>\n<pre><code># 设置ITEM_PIPELINES\nITEM_PIPELINES = {\n   # &apos;scrapy.pipelines.images.ImagesPipeline&apos;:301,\n   &apos;doutu.pipelines.DouTuImagePipelines&apos;: 300,\n}\n# 改变图片下载的路径\nIMAGES_STORE = &apos;images&apos;\n</code></pre>","categories":[],"tags":[{"name":"Python","path":"api/tags/Python.json"},{"name":"框架","path":"api/tags/框架.json"}]}